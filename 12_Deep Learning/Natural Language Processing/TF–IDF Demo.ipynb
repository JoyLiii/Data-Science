{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 關鍵詞提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF 詞頻 (Term Frequency): \n",
    "是指某一個文字在某一個文件中所出現的頻率。\n",
    "\n",
    "可用式子表示為 tf(t,d): t詞在d文件中出現的頻率。\n",
    "\n",
    "例如：十萬 / 青年 / 十萬 / 肝， 十萬出現了2次，總共有3個字詞，\n",
    "\n",
    "因此tf(t,d) = 2/3 = 0.66666。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF 逆向文件頻率 (Inverse Document Frequent):定義為文件數除以某特定文字有被多少文件所提及的數量取log\n",
    "\n",
    "可用式字表示為 idf(t,D) = log (D/dt)\n",
    "\n",
    "(D為文件總數，dt為提及t文字的文件數量)\n",
    "\n",
    "例如：總共有10個文件，其中有5個文件提及“十萬“，則IDF為 log(10/5) = 0.3。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = [\"The sky is blue\",\n",
    "          \"The sky is not blue\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:\n",
      "['blue', 'is', 'not', 'sky', 'the']\n",
      "5\n",
      "Vectorized corpus:\n",
      "[[1 1 0 1 1]\n",
      " [1 1 1 1 1]]\n",
      "index of `的` is : 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# step 1\n",
    "vectoerizer = CountVectorizer(min_df=1, max_df=1.0, token_pattern='\\\\b\\\\w+\\\\b')\n",
    "# step 2\n",
    "vectoerizer.fit(corpus2)\n",
    "# step 3\n",
    "bag_of_words = vectoerizer.get_feature_names()\n",
    "print(\"Bag of words:\")\n",
    "print(bag_of_words)\n",
    "print(len(bag_of_words))\n",
    "# step 4\n",
    "X = vectoerizer.transform(corpus2)\n",
    "print(\"Vectorized corpus:\")\n",
    "print(X.toarray())\n",
    "# step 5\n",
    "print(\"index of `的` is : {}\".format(vectoerizer.vocabulary_.get('not')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue\t1.0\n",
      "is\t1.0\n",
      "not\t1.4054651081081644\n",
      "sky\t1.0\n",
      "the\t1.0\n",
      "[[0.5        0.5        0.         0.5        0.5       ]\n",
      " [0.4090901  0.4090901  0.57496187 0.4090901  0.4090901 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# step 1\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "# step 2\n",
    "tfidf_transformer.fit(X.toarray())\n",
    "# step 3\n",
    "for idx, word in enumerate(vectoerizer.get_feature_names()):\n",
    "  print(\"{}\\t{}\".format(word, tfidf_transformer.idf_[idx]))\n",
    "# step 4\n",
    "tfidf = tfidf_transformer.transform(X)\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有人可以用公式來算出上面這一個矩陣嗎？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 較長的範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"幫我 查下 明天 北京 天氣 怎麼樣\",\n",
    "          \"幫我 查下 今天 北京 天氣 好不好\",\n",
    "          \"幫我 查詢 去 北京 的 火車\",\n",
    "          \"幫我 查看 到 上海 的 火車\",\n",
    "          \"幫我 查看 特朗普 的 新聞\",\n",
    "          \"幫我 看看 有沒有 北京 的 新聞\",\n",
    "          \"幫我 搜索 上海 有 什麼 好玩的\",\n",
    "          \"幫我 找找 上海 東方明珠 在哪\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:\n",
      "['上海', '什麼', '今天', '到', '北京', '去', '在哪', '天氣', '好不好', '好玩的', '幫我', '怎麼樣', '找找', '搜索', '新聞', '明天', '有', '有沒有', '東方明珠', '查下', '查看', '查詢', '火車', '特朗普', '的', '看看']\n",
      "26\n",
      "Vectorized corpus:\n",
      "[[0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
      " [1 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n",
      "index of `的` is : 24\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# step 1\n",
    "vectoerizer = CountVectorizer(min_df=1, max_df=1.0, token_pattern='\\\\b\\\\w+\\\\b')\n",
    "# step 2\n",
    "vectoerizer.fit(corpus)\n",
    "# step 3\n",
    "bag_of_words = vectoerizer.get_feature_names()\n",
    "print(\"Bag of words:\")\n",
    "print(bag_of_words)\n",
    "print(len(bag_of_words))\n",
    "# step 4\n",
    "X = vectoerizer.transform(corpus)\n",
    "print(\"Vectorized corpus:\")\n",
    "print(X.toarray())\n",
    "# step 5\n",
    "print(\"index of `的` is : {}\".format(vectoerizer.vocabulary_.get('的')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上海\t1.8109302162163288\n",
      "什麼\t2.504077396776274\n",
      "今天\t2.504077396776274\n",
      "到\t2.504077396776274\n",
      "北京\t1.587786664902119\n",
      "去\t2.504077396776274\n",
      "在哪\t2.504077396776274\n",
      "天氣\t2.09861228866811\n",
      "好不好\t2.504077396776274\n",
      "好玩的\t2.504077396776274\n",
      "幫我\t1.0\n",
      "怎麼樣\t2.504077396776274\n",
      "找找\t2.504077396776274\n",
      "搜索\t2.504077396776274\n",
      "新聞\t2.09861228866811\n",
      "明天\t2.504077396776274\n",
      "有\t2.504077396776274\n",
      "有沒有\t2.504077396776274\n",
      "東方明珠\t2.504077396776274\n",
      "查下\t2.09861228866811\n",
      "查看\t2.09861228866811\n",
      "查詢\t2.504077396776274\n",
      "火車\t2.09861228866811\n",
      "特朗普\t2.504077396776274\n",
      "的\t1.587786664902119\n",
      "看看\t2.504077396776274\n",
      "[[0.         0.         0.         0.         0.3183848  0.\n",
      "  0.         0.42081614 0.         0.         0.20052115 0.50212047\n",
      "  0.         0.         0.         0.50212047 0.         0.\n",
      "  0.         0.42081614 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.50212047 0.         0.3183848  0.\n",
      "  0.         0.42081614 0.50212047 0.         0.20052115 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.42081614 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.33116919 0.52228256\n",
      "  0.         0.         0.         0.         0.20857285 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.52228256 0.43771355 0.\n",
      "  0.33116919 0.        ]\n",
      " [0.38715525 0.         0.         0.53534183 0.         0.\n",
      "  0.         0.         0.         0.         0.21378805 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.44865824 0.         0.44865824 0.\n",
      "  0.33944982 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.23187059 0.\n",
      "  0.         0.         0.48660646 0.         0.         0.\n",
      "  0.         0.         0.48660646 0.         0.         0.5806219\n",
      "  0.36816103 0.        ]\n",
      " [0.         0.         0.         0.         0.33116919 0.\n",
      "  0.         0.         0.         0.         0.20857285 0.\n",
      "  0.         0.         0.43771355 0.         0.         0.52228256\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33116919 0.52228256]\n",
      " [0.33420711 0.4621274  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.4621274  0.18454996 0.\n",
      "  0.         0.4621274  0.         0.         0.4621274  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.37686288 0.         0.         0.         0.         0.\n",
      "  0.52110999 0.         0.         0.         0.20810458 0.\n",
      "  0.52110999 0.         0.         0.         0.         0.\n",
      "  0.52110999 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# step 1\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "# step 2\n",
    "tfidf_transformer.fit(X.toarray())\n",
    "# step 3\n",
    "for idx, word in enumerate(vectoerizer.get_feature_names()):\n",
    "  print(\"{}\\t{}\".format(word, tfidf_transformer.idf_[idx]))\n",
    "# step 4\n",
    "tfidf = tfidf_transformer.transform(X)\n",
    "print(tfidf.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
